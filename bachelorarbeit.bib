@misc{trabs,
    author = "Trabs, M.",
    title = "({Vorlesungsskript}): {The Mathematics of Machine Learning}",
    howpublished = {\url{https://www.math.uni-hamburg.de/home/trabs/Lehre/MathMachLearnSS19.pdf}},
    year = {2019},
    institution = {Universit√§t Hamburg},
    note = {Besucht: {28.02.2021}}
}

@misc{lotz,
    author = "Lotz, M.",
    title = "({Vorlesungsskript}): {Mathematics of Machine Learning}",
    howpublished = {\url{http://homepages.warwick.ac.uk/staff/Martin.Lotz/files/learning/lectnotes-all.pdf}},
    month = {March},
    year = {2020},
    institution = {University of Warwick},
    note = {Besucht: {28.02.2021}},
}

@book{svm,
    author = {Christmann, A. and Steinwart, I.},
    title = {Support Vector Machines},
    isbn = {978-0-387-77241-7},
    series = {Information Science and Statistics},
    year = {2008},
    publisher = {Springer},
}

@book{foundations,
    author = {Mohri, M. and Rostamizadeh, A. and Talwalkar, A.},
    title = {Foundations of Machine Learning},
    description = {Second edition},
    series = {Adaptive computation and machine learning series},
    isbn = {9780262039406},
    year = {2018},
    publisher = {The MIT Press},
}

@book{understanding,
    author = {Ben-David, S. and Shalev-Shwarz, S.},
    title = {Understanding Machine Learning: From Theory to Algorithms},
    isbn = {978-1-107-05713-5},
    year = {2014},
    publisher = {Cambridge University Press}
}

@book{fundamentals,
    author = {Ernst, F. and Schweikard, A.},
    title = {Fundamentals of Machine Learning},
    subtitle = {Support Vector Machines Made Easy},
    isbn = {978-3-8385-5251-4},
    year = {2020},
    publisher = {UVK Verlag},
}


@book{ledoux_probability_2011,
	address = {Berlin ; London},
	series = {Classics in mathematics},
	title = {Probability in {Banach} spaces: isoperimetry and processes},
	isbn = {9783642202117 9783642202124},
	shorttitle = {Probability in {Banach} spaces},
	publisher = {Springer},
	author = {Ledoux, M. and Talagrand, M.},
	year = {2011},
	note = {OCLC: ocn751525992},
	keywords = {Probabilities, Banach spaces},
}


@article{blanchard_statistical_2008,
	title = {Statistical performance of support vector machines},
	url = {http://arxiv.org/abs/0804.0551},
	doi = {10.1214/009053607000000839},
	abstract = {The support vector machine (SVM) algorithm is well known to the computer learning community for its very good practical results. The goal of the present paper is to study this algorithm from a statistical perspective, using tools of concentration theory and empirical processes. Our main result builds on the observation made by other authors that the SVM can be viewed as a statistical regularization procedure. From this point of view, it can also be interpreted as a model selection principle using a penalized criterion. It is then possible to adapt general methods related to model selection in this framework to study two important points: (1) what is the minimum penalty and how does it compare to the penalty actually used in the SVM algorithm; (2) is it possible to obtain ``oracle inequalities'' in that setting, for the specific loss function used in the SVM algorithm? We show that the answer to the latter question is positive and provides relevant insight to the former. Our result shows that it is possible to obtain fast rates of convergence for SVMs.},
	urldate = {2021-03-16},
	journal = {arXiv:0804.0551 [math, stat]},
	author = {Blanchard, Gilles and Bousquet, Olivier and Massart, Pascal},
	month = apr,
	year = {2008},
	note = {arXiv: 0804.0551},
	keywords = {Mathematics - Statistics Theory, 62G05, 62G20 (Primary)},
}

@article{STEINWART2021101513,
title = {A closer look at covering number bounds for Gaussian kernels},
journal = {Journal of Complexity},
volume = {62},
pages = {101513},
year = {2021},
issn = {0885-064X},
doi = {https://doi.org/10.1016/j.jco.2020.101513},
url = {https://www.sciencedirect.com/science/article/pii/S0885064X2030056X},
author = {Ingo Steinwart and Simon Fischer},
keywords = {Gaussian kernels, Covering numbers},
abstract = {We establish some new bounds on the log-covering numbers of (anisotropic) Gaussian reproducing kernel Hilbert spaces. Unlike previous results in this direction we focus on small explicit constants and their dependency on crucial parameters such as the kernel bandwidth and the size and dimension of the underlying space.}
}